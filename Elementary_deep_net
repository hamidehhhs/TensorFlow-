import tensorflow as tf
from sklearn import datasets , model_selection

deep_learning = tf.constant('Deep Learning')

type(deep_learning)

This is just simple Python code not tensorflow 

deep_learning=1

deep_learning

To Run deep algorithms need session 

session = tf.Session()

#session.run(deep_learning)

Multiply operator in tensorflow

a=tf.constant(2)

b=tf.constant(3)

multiply = tf.multiply(a, b)

session.run(multiply)

Define variables

weights = tf.Variable(tf.random_normal([300, 200], stddev=0.5),
                          name="weights")

init=tf.initialize_all_variables()

session.run(init)

session.run(weights)

Construct a matrix multiplication XW

x = tf.placeholder(tf.float32, name="x", shape=[None, 784])

Placeholder make a variable without any value. 

W = tf.Variable(tf.random_uniform([784,10], -1, 1), name="W")

multiply = tf.matmul(x, W)

Run a new model 

data=datasets.load_digits()

XX=data.data

Y=data.target

Y

y_tensor=tf.one_hot(Y,depth=10)

y_t=session.run(y_tensor)

y_t.shape

XX.shape

Define a single layer

x_tr,x_te,y_tr,y_te=model_selection.train_test_split(XX,y_t)



def layer(X,w_shape,b_shape):
    w=tf.get_variable(name='w',shape=w_shape,initializer=tf.random_uniform_initializer())
    b=tf.get_variable(name='b',shape=b_shape,initializer=tf.random_uniform_initializer())
    z=tf.nn.relu(tf.matmul(X,w)+b)
    return z

get_variable will use the existence variables but variable each time will create a new variable 

Construct a 4 layer networks

def neron(X):
    with tf.variable_scope('Hidden0'):
        Hidden0=layer(X,[64,128],128)
    with tf.variable_scope('Hidden1'):
        Hidden1=layer(Hidden0,[128,64],64)
    with tf.variable_scope('Hidden2'):
        Hidden2=layer(Hidden1,[64,32],32)
    with tf.variable_scope('Hidden3'):
        Hidden3=layer(Hidden2,[32,10],10)
    return Hidden3

Should use reuse=True  in the second run of algorithm but should be False (or not use) in first run

def loss(y_predict,y):
    entropy=tf.nn.softmax_cross_entropy_with_logits(logits=y_predict,labels=y)
    loss=tf.reduce_mean(entropy)
    return loss

Define Optimizer function 

def optimizer(cost):
    #optimize=tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)
    optimize=tf.train.AdagradOptimizer(learning_rate=learning_rate,global=1).minimize(cost)
    return optimize

def evaluate(output,y):
    #compare the maximum value in each row of two input vectors 
    predict=tf.equal(tf.argmax(output,1),tf.argmax(y,1))
    #cast function change Bolean value to 0-1
    ac=tf.reduce_mean(tf.cast(predict,tf.float32))
    return ac

#None make the number of row to be vary 
x=tf.placeholder('float',[None,64])
y=tf.placeholder('float',[None,10])
epoch=20000
learning_rate=0.1

output=neron(x)
cost=loss(output,y)
train=optimizer(cost)
eval_ = evaluate(output, y)

init_op = tf.global_variables_initializer()



session.run(init_op)



feed_dict1 = {x : x_tr, y : y_tr}
feed_dict2 = {x : x_te, y : y_te}

for i in range(epoch):
    session.run(train,feed_dict1)

print('accuracy _train: ',session.run(eval_,feed_dict1))
print('accuracy _test: ',session.run(eval_,feed_dict2))

